{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42935b2c-ed88-4ae6-9bc8-2fad7a66858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b59141d-8ec0-4baf-af25-c16efdb462b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined feature and label arrays\n",
    "X = np.concatenate([np.load('X_comcuc.npy'), np.load('X_cowpig1.npy'),\n",
    "                     np.load('X_eucdov.npy'), np.load('X_eueowl1.npy'),\n",
    "                     np.load('X_grswoo.npy'), np.load('X_tawowl1.npy')])\n",
    "\n",
    "y = np.concatenate([np.load('comcuc_combined_labels.npy'), np.load('cowpig1_combined_labels.npy'),\n",
    "                     np.load('eucdov_combined_labels.npy'), np.load('eueowl1_combined_labels.npy'),\n",
    "                     np.load('grswoo_combined_labels.npy'), np.load('tawowl1_combined_labels.npy')])\n",
    "\n",
    "# Combine the feature and label arrays into a single dataset\n",
    "dataset = np.hstack((X, y.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93e75f0-a8e3-4cf4-9672-66ca5c348962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique labels in the dataset\n",
    "num_labels = len(np.unique(y))\n",
    "\n",
    "# Initialize an empty list to store the stratified samples\n",
    "stratified_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8811101-f520-4439-bebd-293a434114f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each label\n",
    "for label in range(num_labels):\n",
    "    # Get the indices of instances with the current label\n",
    "    indices = np.where(y == label)[0]\n",
    "    \n",
    "    # Get the number of instances with the current label\n",
    "    num_instances = len(indices)\n",
    "    \n",
    "    # Split the instances into training and validation sets using stratified sampling\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.5, stratify=y[indices])\n",
    "    \n",
    "    # Add the training instances to the stratified sample list\n",
    "    stratified_samples.append(train_indices)\n",
    "    \n",
    "# Concatenate the stratified samples into a single list of indices\n",
    "stratified_indices = np.concatenate(stratified_samples)\n",
    "\n",
    "# Use the stratified indices to select a subset of instances from the dataset\n",
    "subset_dataset = dataset[stratified_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59e89d6a-6c94-4a54-902c-98eacdc8e8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59999, 255)\n"
     ]
    }
   ],
   "source": [
    "print(subset_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f3567cd-cbd4-4236-8461-de4a8c642024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X = subset_dataset[:, :-1]  # Features\n",
    "y = subset_dataset[:, -1]   # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b56a3acd-ef84-44d9-a2b7-53a8e9aac4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier()\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "578e05d2-7fc4-46d1-b9a4-8944a3e34e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.8973483179153818\n",
      "Average F1 score: 0.8006921301430111\n",
      "Average training accuracy: 0.999729161979069\n",
      "Average training F1 score: 0.9994850675582849\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c90fa17a-9800-489c-bd89-2bde1e522da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier()\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 10\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d8e1550-33b1-4665-a139-de671a61d705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.9011816747235649\n",
      "Average F1 score: 0.8084369876724053\n",
      "Average training accuracy: 0.9997296250685025\n",
      "Average training F1 score: 0.9996400954325411\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95a2ee60-ef42-443a-9852-59196ab4ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier(alpha=0.01)\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c1fc091-1f59-4091-9cc7-d27b48499ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.8990150206961692\n",
      "Average F1 score: 0.8042310550062298\n",
      "Average training accuracy: 0.9996958277776621\n",
      "Average training F1 score: 0.9996345553469828\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb2afb90-1e46-4719-9215-79e654438dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier(alpha=1)\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79e338b1-8293-446c-8f1c-e10de52748c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.911348540156124\n",
      "Average F1 score: 0.8203034176255903\n",
      "Average training accuracy: 0.9398448284512873\n",
      "Average training F1 score: 0.8783286200648023\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d00739b-0fb6-4eaa-b844-7f3769af5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier(alpha=10)\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cda5b1db-8d0c-4a4f-b37c-c0537a9e5cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.8684311637080867\n",
      "Average F1 score: 0.6815909694653289\n",
      "Average training accuracy: 0.8713603496079779\n",
      "Average training F1 score: 0.6876611606859893\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae86f7bb-e7b9-4c37-aedb-d14816538da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50))\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e5be88b-d05b-4ce3-95e6-6f3728c5fc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.9022816665277661\n",
      "Average F1 score: 0.8121034954261823\n",
      "Average training accuracy: 0.9998333307291125\n",
      "Average training F1 score: 0.9998107879159586\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "072e0cf3-380e-4442-9328-6a5b759a5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 50))\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f1f8441-84bc-4789-8ddd-225a92f5c41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.8992315929105203\n",
      "Average F1 score: 0.8049075540202008\n",
      "Average training accuracy: 0.9970957901900734\n",
      "Average training F1 score: 0.9948547121265833\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3d28f8c-bcfd-4c4c-9601-519d52f6fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100))\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e0a3bba-b106-48be-9f7b-d45d42c2711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.8995816429146872\n",
      "Average F1 score: 0.8069268859950898\n",
      "Average training accuracy: 0.9967666211796079\n",
      "Average training F1 score: 0.9937670246610851\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "900fe9fb-c53a-48f8-a1dc-6f11d523a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier with default parameters\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50), alpha=1, solver='adam', activation='relu', \n",
    "                    learning_rate='adaptive', max_iter=200)\n",
    "\n",
    "# define the number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# initialize lists to store the results\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "train_acc_scores = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# initialize the KFold object\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd399246-a88c-4ea2-8765-813ae2ea27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "Average accuracy: 0.9104818554323971\n",
      "Average F1 score: 0.823201031677069\n",
      "Average training accuracy: 0.9574992892213032\n",
      "Average training F1 score: 0.9124209346160888\n"
     ]
    }
   ],
   "source": [
    "# loop over the folds\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "\n",
    "    # split the data into training and testing sets\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # fit the classifier to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels of the testing data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    # predict the labels of the training data\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "\n",
    "    # compute the accuracy and F1 score of the predictions on the training data\n",
    "    train_acc_scores.append(accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(f1_score(y_train, y_train_pred, average='macro'))\n",
    "    print(1)\n",
    "# print the average accuracy and F1 score across the folds\n",
    "print(\"Average accuracy:\", sum(acc_scores)/n_folds)\n",
    "print(\"Average F1 score:\", sum(f1_scores)/n_folds)\n",
    "\n",
    "# print the average training accuracy and F1 score across the folds\n",
    "print(\"Average training accuracy:\", sum(train_acc_scores)/n_folds)\n",
    "print(\"Average training F1 score:\", sum(train_f1_scores)/n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa07a9-63bf-4915-b5e4-8d885da414e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
